{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from falkon import LogisticFalkon\n",
    "from falkon.kernels import GaussianKernel\n",
    "from falkon.options import FalkonOptions\n",
    "from falkon.gsc_losses import WeightedCrossEntropyLoss\n",
    "\n",
    "from scipy.spatial.distance import pdist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logflk_config(M,flk_sigma,lam,weight,iter=[1000000],seed=None,cpu=False):\n",
    "    # it returns logfalkon parameters\n",
    "    return {\n",
    "            'kernel' : GaussianKernel(sigma=flk_sigma),\n",
    "            'M' : M, #number of Nystrom centers,\n",
    "            'penalty_list' : [lam], # list of regularization parameters,\n",
    "            'iter_list' : [iter], #list of number of CG iterations,\n",
    "            'options' : FalkonOptions(cg_tolerance=np.sqrt(1e-7), keops_active='no', use_cpu=cpu, debug = False),\n",
    "            'seed' : seed, # (int or None), the model seed (used for Nystrom center selection) is manually set,\n",
    "            'loss' : WeightedCrossEntropyLoss(kernel=GaussianKernel(sigma=flk_sigma), neg_weight=weight),\n",
    "            }\n",
    "\n",
    "def trainer(X,Y,flk_config):\n",
    "    # trainer for logfalkon model\n",
    "    Xtorch=torch.from_numpy(X)\n",
    "    Ytorch=torch.from_numpy(Y)\n",
    "    model = LogisticFalkon(**flk_config)\n",
    "    model.fit(Xtorch, Ytorch)\n",
    "    return model.predict(Xtorch).numpy()\n",
    "\n",
    "def compute_t(preds,Y,weight):\n",
    "    # it returns extended log likelihood ratio from predictions\n",
    "    diff = weight*np.sum(1 - np.exp(preds[Y==0]))\n",
    "    return 2 * (diff + np.sum(preds[Y==1]))\n",
    "\n",
    "\n",
    "def candidate_sigma(data, perc=90):\n",
    "    # this function gives an estimate of the width of the gaussian kernel\n",
    "    # use on a (small) sample of reference data (standardize first if necessary)\n",
    "    pairw = pdist(data)\n",
    "    return round(np.percentile(pairw,perc),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_0 = 20000 # size of reference sample\n",
    "N_1 = 2000 # size of the data sample\n",
    "N = N_0 + N_1\n",
    "weight = N_1/N_0\n",
    "\n",
    "M = 1000\n",
    "lam = 1e-6\n",
    "# sigma is set later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Gaussian width: 3.9\n"
     ]
    }
   ],
   "source": [
    "# generate data\n",
    "\n",
    "rng = np.random.default_rng(seed=0)\n",
    "\n",
    "scale = 1.2 # if scale = 0 we are in the null hypothesis \n",
    "dim = 4\n",
    "\n",
    "# estimate the gaussian width on a reference sample (not too large, the pairwise distance is expensive to compute)\n",
    "\n",
    "ref_sample_for_sigma = rng.multivariate_normal(mean=dim*[1],cov=np.diag(np.full(dim,1)),size=1000)\n",
    "\n",
    "flk_sigma = candidate_sigma(ref_sample_for_sigma, perc=90)\n",
    "print(f\"Estimated Gaussian width: {flk_sigma}\")\n",
    "\n",
    "\n",
    "X = np.zeros(shape=(N,dim))\n",
    "\n",
    "X[:N_0,:] = rng.multivariate_normal(mean=dim*[1],cov=np.diag(np.full(dim,1)),size=N_0) # reference\n",
    "X[N_0:,:] = rng.multivariate_normal(mean=dim*[scale],cov=np.diag(np.full(dim,1)),size=N_1) # data\n",
    "\n",
    "\n",
    "# initialize labes\n",
    "Y = np.zeros(shape=(N,1))\n",
    "Y[N_0:,:] = np.ones((N_1,1)) # flip data labels to one for data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - penalty 1.000000e-06 - sub-iterations 1000000\n",
      "Value of the test statistic: 364.617466781465\n",
      "Training and evaluation time: 0.38\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate\n",
    "flk_config = get_logflk_config(M,flk_sigma,lam,weight=weight,iter=1000000,seed=None,cpu=False)\n",
    "flk_config['seed'] = 0 # seed for center selection\n",
    "\n",
    "st_time = time.time()\n",
    "preds = trainer(X,Y,flk_config)\n",
    "\n",
    "t = compute_t(preds,Y,weight)\n",
    "dt = round(time.time()-st_time,2)\n",
    "\n",
    "print(f\"Value of the test statistic: {t}\\nTraining and evaluation time: {dt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('FalkonHEPv2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4e59eef8e934b1fba1636c11e3f20efca9f45a170fe6b949027d08e3bf84862"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
